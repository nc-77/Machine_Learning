{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loadDataSet:创建训练数据\n",
    "\n",
    "dataset:训练文档\n",
    "\n",
    "classVec:文档依次对应标签(0表示文明/1表示不文明)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    dataset=[['my','dog','has','flea',\n",
    "                  'problems','help','please'],\n",
    "                 ['maybe','not','take','him',\n",
    "                  'to','dog','park','stupid'],\n",
    "                 ['my','dalmation','is','so','cute',\n",
    "                  'I','love','him'],\n",
    "                 ['stop','posting','stupid','worthless','garbage'],\n",
    "                 ['my','licks','ate','my','steak','how',\n",
    "                  'to','stop','him'],\n",
    "                 ['quit','buying','worthless','dog','food','stupid']]\n",
    "    classVec=[0,1,0,1,0,1]\n",
    "    return dataset,classVec\n",
    "trainData,trainclass=loadDataSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "createVocab:创建词汇表\n",
    "\n",
    "trainData:训练文档\n",
    "\n",
    "Vocab:由训练文档中所有单词组成的词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cute', 'stop', 'park', 'posting', 'ate', 'has', 'dog', 'problems', 'take', 'him', 'please', 'my', 'steak', 'love', 'worthless', 'licks', 'not', 'how', 'so', 'garbage', 'quit', 'is', 'to', 'maybe', 'dalmation', 'help', 'food', 'stupid', 'I', 'flea', 'buying']\n"
     ]
    }
   ],
   "source": [
    "def createVocab(trainData):\n",
    "    Vocab=set([])\n",
    "    for document in trainData:\n",
    "        Vocab=Vocab|set(document)\n",
    "    return list(Vocab)\n",
    "VocabList=createVocab(trainData)\n",
    "print(VocabList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "createWordsVec:将inX转换成词向量\n",
    "\n",
    "VocabList:创建好的词汇表\n",
    "\n",
    "inX:需要转换的文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createWordsVec(VocabList,inX):\n",
    "    outX=[0]*len(VocabList)\n",
    "    for word in inX:\n",
    "        if(word in VocabList):\n",
    "            outX[VocabList.index(word)]+=1\n",
    "    return outX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bayesTrain:训练朴素贝叶斯分类器\n",
    "\n",
    "trainMat:将训练文档转换为词向量组成的矩阵\n",
    "\n",
    "trainclass:训练文档对应的标签\n",
    "\n",
    "p0v:p(w|0)组成的向量\n",
    "\n",
    "p1v:p(w|1)组成的向量\n",
    "\n",
    "pAbusive:文档属于侮辱类的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def bayesTrain(trainMat,trainclass):\n",
    "    n=len(trainMat[0])\n",
    "    pAbusive=sum(trainclass)/len(trainMat)\n",
    "    p0v=np.ones(n)\n",
    "    p1v=np.ones(n)\n",
    "    sum0=sum1=n\n",
    "    for i in range(len(trainMat)):\n",
    "        if(trainclass[i]):\n",
    "            p1v+=trainMat[i]\n",
    "            sum1+=sum(trainMat[i])\n",
    "        else:\n",
    "            p0v+=trainMat[i]\n",
    "            sum0+=sum(trainMat[i])\n",
    "    p0v=np.log(p0v/sum0)\n",
    "    p1v=np.log(p1v/sum1)\n",
    "    return p0v,p1v,pAbusive\n",
    "trainMat=[]\n",
    "for document in trainData:\n",
    "    trainMat.append(createWordsVec(VocabList,document))\n",
    "p0v,p1v,pAbusive=bayesTrain(trainMat,trainclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classify:分类测试\n",
    "\n",
    "inX:待分类的向量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love,my,dalmation classified as :0\n",
      "stupid,garbage classified as :1\n"
     ]
    }
   ],
   "source": [
    "def classify(inX,p0v,p1v,pclass1):\n",
    "    p1=sum(inX*p1v)+log(pclass1)\n",
    "    p0=sum(inX*p0v)+log(1-pclass1)\n",
    "    if(p1>p0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "test=[['love','my','dalmation'],\n",
    "          ['stupid','garbage']]\n",
    "for document in test:\n",
    "    label=classify(createWordsVec(VocabList,document),p0v,p1v,pAbusive)\n",
    "    print(f\"{','.join(document)} classified as :{label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
